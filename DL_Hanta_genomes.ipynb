{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='honeydew';this.parentNode.removeChild(this)\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n",
      "Going to download record 1 to 20\n",
      "Going to download record 21 to 40\n",
      "Going to download record 41 to 60\n",
      "Going to download record 61 to 80\n",
      "Going to download record 81 to 100\n",
      "Going to download record 101 to 120\n",
      "Going to download record 121 to 140\n",
      "Going to download record 141 to 160\n",
      "Going to download record 161 to 180\n",
      "Going to download record 181 to 200\n",
      "Going to download record 201 to 220\n",
      "Going to download record 221 to 240\n",
      "Going to download record 241 to 260\n",
      "Going to download record 261 to 280\n",
      "Going to download record 281 to 300\n",
      "Going to download record 301 to 320\n",
      "Going to download record 321 to 339\n",
      "339\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#This code is from download_genomes_Entrez in /jupyter\n",
    "\n",
    "#Download all hanta genomes, all segments\n",
    "\n",
    "\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.api_key = \"<enter_your_api_key_here>\"\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "Entrez.email = \"<enter_your_email_here>\"  # Always tell NCBI who you are\n",
    "handle = Entrez.esearch(db=\"nucleotide\", term=\"Sin Nombre orthohantavirus[Primary Organism]\", idtype=\"acc\", usehistory=\"y\", retmax=10000)\n",
    "record= Entrez.read(handle)\n",
    "\n",
    "webenv = record[\"WebEnv\"]\n",
    "query_key = record[\"QueryKey\"]\n",
    "\n",
    "\n",
    "count = record[\"Count\"]\n",
    "count=int(count)\n",
    "idlist = \", \".join(record[\"IdList\"][:count])\n",
    "print(count)\n",
    "\n",
    "#Download xml files from Genbank\n",
    "batch_size = 20\n",
    "\n",
    "all_data=[]\n",
    "\n",
    "for start in range(0, count, batch_size):\n",
    "    end = min(count, start + batch_size)\n",
    "    print(\"Going to download record %i to %i\" % (start + 1, end))\n",
    "    fetch_handle = Entrez.efetch(\n",
    "        db=\"nucleotide\",\n",
    "        retmode=\"xml\",\n",
    "        id=idlist,\n",
    "        retstart=start,\n",
    "        retmax=batch_size,\n",
    "        webenv=webenv,\n",
    "    )\n",
    "    datum=Entrez.read(fetch_handle)\n",
    "    all_data.extend(datum)\n",
    "\n",
    "fetch_handle.close()\n",
    "\n",
    "print(len(all_data))\n",
    "\n",
    "df_columns=['accession_num', 'strain', 'isolate', 'segment', 'chromosome', 'sequence_length', 'host', 'location', 'subtype', 'organism', 'collection_date', 'sequence']\n",
    "sigma=15\n",
    "df_Compiled_Data = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "#all_data is a list that's compiled from the esearch function results in cell #163.\n",
    "#for loop below iterates through all of the returned search results.\n",
    "for gb_entry in range(0, len(all_data)):\n",
    "    #print(datum[gb_entry])\n",
    "    accession_number=\"\"\n",
    "    strain=\"\"\n",
    "    isolate=\"\"\n",
    "    segment=\"\"\n",
    "    chromosome=\"\"\n",
    "    sequence_length=\"\"\n",
    "    host=\"\"\n",
    "    location=\"\"\n",
    "    subtype=\"\"\n",
    "    organism=\"\"\n",
    "    date=\"\"\n",
    "    sequence=\"\"\n",
    "    \n",
    "    #Grab accession number.\n",
    "    #print(all_data[gb_entry][\"GBSeq_locus\"])\n",
    "    accession_number=all_data[gb_entry][\"GBSeq_locus\"]\n",
    "    \n",
    "    #Grab sequence.\n",
    "    sequence=all_data[gb_entry][\"GBSeq_sequence\"]\n",
    "    #print(sequence)\n",
    "    \n",
    "    #Iterate through all of the GBFeature_quals and process the data:\n",
    "    for item in range(0, (len(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"]))):\n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"strain\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            strain=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"isolate\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            isolate=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"segment\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            segment=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"chromosome\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            chromosome=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"host\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            host=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "        \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"country\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            location=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"subtype\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            subtype=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"organism\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            organism=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "            \n",
    "        #if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"collection_date\"):\n",
    "        if(re.search(\"collection_date\", all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"], re.IGNORECASE) is not None):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "            date=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "        \n",
    "        #if(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_name\"] == \"date\"):\n",
    "            #print(all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"])\n",
    "        #   date=all_data[gb_entry][\"GBSeq_feature-table\"][0][\"GBFeature_quals\"][item][\"GBQualifier_value\"]\n",
    "    \n",
    "    #print(accession_number)\n",
    "    #print(organism)\n",
    "    #print(strain)\n",
    "    #print(segment)\n",
    "    #print(chromosome)\n",
    "    #print(sequence)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    temp_df = pd.DataFrame({'accession_num':accession_number,\n",
    "                            'strain':strain,\n",
    "                            'isolate':isolate,\n",
    "                            'segment':segment, \n",
    "                            'chromosome':chromosome, \n",
    "                            'sequence_length':len(sequence),\n",
    "                            'host':host,\n",
    "                            'location':location,\n",
    "                            'subtype':subtype,\n",
    "                            'organism':organism,\n",
    "                            'collection_date':date,\n",
    "                            'sequence':sequence}, index=[gb_entry])\n",
    "    \n",
    "    df_Compiled_Data=df_Compiled_Data.append(temp_df)\n",
    "            \n",
    "    #print(\"\\n\")\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "df_Compiled_Data.to_excel(\"/PATH/save_to_excel_file.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='honeydew';this.parentNode.removeChild(this)\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All L's downloaded!\n"
     ]
    }
   ],
   "source": [
    "#This script will parse the contents of all available hanta sequences and save a fasta file for hanta, L segments of a certain size.\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict \n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "output_handle = open(\"/PATH/SNV_all_L.fasta\", \"w\")\n",
    "\n",
    "all_data_dictionary=OrderedDict()\n",
    "#Assign contents of pandas df to a dictionary:\n",
    "for index_df, row_df in df_Compiled_Data.iterrows():\n",
    "    sequence_name = \"|\".join((row_df['accession_num'], row_df['strain'], row_df['isolate'], row_df['segment'], str(row_df['sequence_length']), row_df['host'], row_df['location'], row_df['subtype'], row_df['organism'], row_df['collection_date']))\n",
    "    all_data_dictionary.update({sequence_name:row_df['sequence']})\n",
    "\n",
    "#Need to access ith items in the ordered dictionary.  Turn them into a list.\n",
    "all_data_dictionary_items=list(all_data_dictionary.items())\n",
    "\n",
    "for index_df, row_df in df_Compiled_Data.iterrows():\n",
    "    \n",
    "    #If correct virus continue parsing data:\n",
    "    if re.search('Sin Nombre orthohantavirus', row_df['organism'], re.IGNORECASE) is not None:\n",
    "        \n",
    "        #If correct virus and L segment and right size, write to fasta.\n",
    "        if re.search(\"L\", row_df['segment'], re.IGNORECASE) is not None and (int(row_df[\"sequence_length\"]) > 6000):\n",
    "            record=SeqRecord(Seq(all_data_dictionary_items[index_df][1]), id=all_data_dictionary_items[index_df][0], description=\"\")\n",
    "            #print(record)\n",
    "            SeqIO.write(record, output_handle, 'fasta-2line')\n",
    "        \n",
    "        #If correct virus and no value in segment and right size, write to fasta.\n",
    "        if re.search(\"L\", row_df['segment'], re.IGNORECASE) is None and (int(row_df[\"sequence_length\"]) > 6000):\n",
    "            record=SeqRecord(Seq(all_data_dictionary_items[index_df][1]), id=all_data_dictionary_items[index_df][0], description=\"\")\n",
    "            #print(record)\n",
    "            SeqIO.write(record, output_handle, 'fasta-2line')\n",
    "\n",
    "output_handle.close()\n",
    "\n",
    "print(\"All L's downloaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='honeydew';this.parentNode.removeChild(this)\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All M's downloaded!\n"
     ]
    }
   ],
   "source": [
    "#This script will parse the contents of all available hanta sequences and save a fasta file for hanta, M segments of a certain size.\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict \n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "output_handle = open(\"/PATH/SNV_all_M.fasta\", \"w\")\n",
    "\n",
    "all_data_dictionary=OrderedDict()\n",
    "#Assign contents of pandas df to a dictionary:\n",
    "for index_df, row_df in df_Compiled_Data.iterrows():\n",
    "    sequence_name = \"|\".join((row_df['accession_num'], row_df['strain'], row_df['isolate'], row_df['segment'], str(row_df['sequence_length']), row_df['host'], row_df['location'], row_df['subtype'], row_df['organism'], row_df['collection_date']))\n",
    "    all_data_dictionary.update({sequence_name:row_df['sequence']})\n",
    "\n",
    "#Need to access ith items in the ordered dictionary.  Turn them into a list.\n",
    "all_data_dictionary_items=list(all_data_dictionary.items())\n",
    "\n",
    "for index_df, row_df in df_Compiled_Data.iterrows():\n",
    "    if re.search('Sin Nombre orthohantavirus', row_df['organism'], re.IGNORECASE) is not None:\n",
    "        \n",
    "        if re.search(\"M\", row_df['segment'], re.IGNORECASE) is not None and (int(row_df[\"sequence_length\"]) > 3000):\n",
    "            record=SeqRecord(Seq(all_data_dictionary_items[index_df][1]), id=all_data_dictionary_items[index_df][0], description=\"\")\n",
    "            #print(record)\n",
    "            SeqIO.write(record, output_handle, 'fasta-2line')\n",
    "            \n",
    "        #If correct virus and no value in segment and right size, write to fasta.\n",
    "        if re.search(\"M\", row_df['segment'], re.IGNORECASE) is None and (int(row_df[\"sequence_length\"]) > 3000) and (int(row_df[\"sequence_length\"]) < 4000):\n",
    "            record=SeqRecord(Seq(all_data_dictionary_items[index_df][1]), id=all_data_dictionary_items[index_df][0], description=\"\")\n",
    "            #print(record)\n",
    "            SeqIO.write(record, output_handle, 'fasta-2line')\n",
    "\n",
    "output_handle.close()\n",
    "\n",
    "print(\"All M's downloaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"var cell = this.closest('.jp-CodeCell');var editor = cell.querySelector('.jp-Editor');editor.style.background='honeydew';this.parentNode.removeChild(this)\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All S's downloaded!\n"
     ]
    }
   ],
   "source": [
    "#This script will parse the contents of all available Hanta sequences and save a fasta file for hanta, S segments of a certain size.\n",
    "\n",
    "import re\n",
    "from collections import OrderedDict \n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "output_handle = open(\"/PATH/SNV_all_S.fasta\", \"w\")\n",
    "\n",
    "all_data_dictionary=OrderedDict()\n",
    "#Assign contents of pandas df to a dictionary:\n",
    "for index_df, row_df in df_Compiled_Data.iterrows():\n",
    "    sequence_name = \"|\".join((row_df['accession_num'], row_df['strain'], row_df['isolate'], row_df['segment'], str(row_df['sequence_length']), row_df['host'], row_df['location'], row_df['subtype'], row_df['organism'], row_df['collection_date']))\n",
    "    all_data_dictionary.update({sequence_name:row_df['sequence']})\n",
    "\n",
    "#Need to access ith items in the ordered dictionary.  Turn them into a list.\n",
    "all_data_dictionary_items=list(all_data_dictionary.items())\n",
    "\n",
    "for index_df, row_df in df_Compiled_Data.iterrows():\n",
    "    if re.search('Sin Nombre orthohantavirus', row_df['organism'], re.IGNORECASE) is not None:\n",
    "        \n",
    "        if re.search(\"S\", row_df['segment'], re.IGNORECASE) is not None and (int(row_df[\"sequence_length\"]) > 1500):\n",
    "            record=SeqRecord(Seq(all_data_dictionary_items[index_df][1]), id=all_data_dictionary_items[index_df][0], description=\"\")\n",
    "            #print(record)\n",
    "            SeqIO.write(record, output_handle, 'fasta-2line')\n",
    "            \n",
    "        #If correct virus and no value in segment and right size, write to fasta.\n",
    "        if re.search(\"S\", row_df['segment'], re.IGNORECASE) is None and (int(row_df[\"sequence_length\"]) > 1500) and (int(row_df[\"sequence_length\"]) < 2200):\n",
    "            record=SeqRecord(Seq(all_data_dictionary_items[index_df][1]), id=all_data_dictionary_items[index_df][0], description=\"\")\n",
    "            #print(record)\n",
    "            SeqIO.write(record, output_handle, 'fasta-2line')\n",
    "\n",
    "\n",
    "output_handle.close()\n",
    "\n",
    "print(\"All S's downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/Bio/Align/__init__.py\n"
     ]
    }
   ],
   "source": [
    "from Bio import Align\n",
    "print(Align.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from Bio import AlignIO\n",
    " \n",
    "input_handle = open(\"/PATH/SNV_all.fasta\", \"r\")\n",
    "output_handle = open(\"/PATH/SNV_all.phy\", \"w\")\n",
    " \n",
    "alignments = AlignIO.parse(input_handle, \"fasta\")\n",
    "AlignIO.write(alignments, output_handle, \"phylip-relaxed\")\n",
    "\n",
    " \n",
    "output_handle.close()\n",
    "input_handle.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
